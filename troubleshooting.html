<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Hyunwook Kim — Troubleshooting Logs</title>

  <meta property="og:type" content="website" />
  <meta property="og:title" content="Hyunwook Kim — Troubleshooting Logs" />
  <meta property="og:description" content="Technical write-ups and study notes." />
  <meta property="og:url" content="https://hyunwookkim418.github.io/troubleshooting.html" />

  <style>
    :root{
      --bg:#0b0f19;
      --panel:#0f172a;
      --panel2:#111827;
      --text:#e5e7eb;
      --muted:#9ca3af;
      --line:rgba(255,255,255,.10);
      --accent:#60a5fa;
      --accent2:#34d399;
      --shadow: 0 18px 50px rgba(0,0,0,.35);
      --radius:18px;
      --max:1120px;
    }
    *{box-sizing:border-box}
    html,body{
      margin:0;padding:0;
      background:
        radial-gradient(1200px 650px at 12% 0%, rgba(96,165,250,.18), transparent 60%),
        radial-gradient(900px 520px at 90% 10%, rgba(52,211,153,.14), transparent 55%),
        var(--bg);
      color:var(--text);
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Arial;
    }
    a{color:inherit;text-decoration:none}
    .wrap{max-width:var(--max); margin:0 auto; padding:22px 18px 70px;}

    /* Topbar */
    .topbar{
      position:sticky; top:0; z-index:50;
      backdrop-filter: blur(10px);
      background: rgba(11,15,25,.70);
      border-bottom:1px solid var(--line);
    }
    .topbar-inner{
      max-width:var(--max);
      margin:0 auto;
      display:grid;
      grid-template-columns: 1fr auto 1fr;
      align-items:center;
      min-height:72px;
      padding:18px 20px;
      gap:12px;
    }
    .brand{
      display:flex; align-items:center; gap:10px;
      font-weight:900; letter-spacing:1px;
      justify-self:start;
      text-transform:uppercase;
      white-space:nowrap;
    }
    .dot{
      width:10px;height:10px;border-radius:50%;
      background:linear-gradient(135deg,var(--accent),var(--accent2));
      box-shadow:0 0 0 4px rgba(96,165,250,.12);
      flex:0 0 auto;
    }
    .nav{
      display:flex; align-items:center; gap:10px; flex-wrap:wrap;
      justify-self:center;
    }
    .pill{
      display:inline-flex; align-items:center; gap:8px;
      padding:11px 18px;
      border:1px solid var(--line);
      border-radius:999px;
      background: rgba(255,255,255,.03);
      font-weight:800;
      font-size:15px;
      letter-spacing:.3px;
      transition: transform .15s ease, border-color .15s ease, background .15s ease;
    }
    .pill:hover{
      transform: translateY(-1px);
      border-color: rgba(96,165,250,.35);
      background: rgba(96,165,250,.08);
    }
    .pill.primary{border-color: rgba(96,165,250,.45); background: rgba(96,165,250,.12);}
    .pill.primary:hover{background: rgba(96,165,250,.18);}
    .sep{width:1px;height:22px;background:var(--line); display:inline-block; margin:0 2px;}

    /* Header */
    .hero{padding:26px 0 8px;}
    h1{margin:10px 0 8px; font-size:40px; line-height:1.08;}
    .sub{color:var(--muted); font-size:15px; line-height:1.65; max-width:900px; margin:0;}

    /* Troubleshooting Logs*/
    .list{
      margin-top:16px;
      display:flex;
      flex-direction:column;
      gap:12px;
    }
    .post{
      background: linear-gradient(180deg, rgba(255,255,255,.05), rgba(255,255,255,.02));
      border:1px solid var(--line);
      border-radius: var(--radius);
      box-shadow: var(--shadow);
      padding:16px 16px 14px;
      transition: transform .18s ease, border-color .18s ease, background .18s ease;
    }
    .post:hover{
      transform: translateY(-2px);
      border-color: rgba(255,255,255,.18);
      background: rgba(96,165,250,.06);
    }
    .row1{
      display:flex;
      align-items:flex-start;
      justify-content:space-between;
      gap:12px;
      flex-wrap:wrap;
    }
    .title{
      margin:0;
      font-size:17px;
      font-weight:900;
      line-height:1.35;
    }
    .meta{
      display:inline-flex;
      align-items:center;
      gap:8px;
      color: rgba(229,231,235,.85);
      font-weight:900;
      font-size:12px;
      padding:7px 10px;
      border-radius:999px;
      border:1px solid rgba(255,255,255,.12);
      background: rgba(15,23,42,.65);
      white-space:nowrap;
    }
    .meta i{width:8px;height:8px;border-radius:50%;background:var(--accent); display:inline-block;}
    .desc{
      margin:10px 0 0;
      color:var(--muted);
      font-size:14px;
      line-height:1.65;
      max-width:980px;
    }
    .actions{
      margin-top:12px;
      display:flex;
      gap:10px;
      flex-wrap:wrap;
    }
    .btn{
      display:inline-flex; align-items:center; gap:8px;
      padding:10px 12px;
      border-radius: 12px;
      border:1px solid rgba(255,255,255,.10);
      background: rgba(255,255,255,.03);
      font-weight:900; font-size:13px;
      color:#f9fafb; /* ← 이 줄만 핵심 */
      transition: transform .15s ease, background .15s ease, border-color .15s ease;
    }
    .btn:hover{
      transform: translateY(-1px);
      border-color: rgba(96,165,250,.35);
      background: rgba(96,165,250,.08);
    }
    .btn.primary{border-color: rgba(96,165,250,.45); background: rgba(96,165,250,.12);}
    .btn.primary:hover{background: rgba(96,165,250,.18);}

    .footer{
      margin-top:26px;
      color: rgba(229,231,235,.55);
      font-size:12px;
      border-top:1px solid rgba(255,255,255,.08);
      padding-top:16px;
    }

    @media (max-width: 700px){
      .topbar-inner{
        grid-template-columns: 1fr;
        justify-items:center;
        row-gap:10px;
      }
      .brand{ justify-self:center; }
      .nav{ justify-self:center; justify-content:center; }
      h1{font-size:32px;}
    }
    
    @media (max-width: 700px){
      .nav{
        display: grid;
        grid-template-columns: 1fr 1fr; /* 2칸 그리드 */
        gap: 10px;
        width: 100%;
        max-width: 520px; /* 너무 넓어 보이면 조절 */
        justify-items: stretch;
      }

      /* 1줄: Network Projects (전체 폭) */
      .nav a:nth-of-type(1){
        grid-column: 1 / -1;
      }

      /* 2줄: Troubleshooting Logs (전체 폭) */
      .nav a:nth-of-type(2){
        grid-column: 1 / -1;
      }

      /* 3줄: Resume(왼쪽), LinkedIn(오른쪽) */
      .nav a:nth-of-type(3){
        grid-column: 1;
      }
      .nav a:nth-of-type(4){
        grid-column: 2;
      }

      .pill{
        justify-content: center;
        width: 100%;
      }

      /* sep는 모바일에서 필요 없음 */
      .sep{ display:none; }
    }
  
  </style>
</head>

<body>

  <!-- Top Bar -->
  <div class="topbar">
    <div class="topbar-inner">
      <div class="brand">
        <span class="dot"></span>
        <span>Hyunwook Kim</span>
      </div>

      <div class="nav">
        <!-- Site -->
        <a class="pill" href="projects.html">Network Projects</a>
        <a class="pill primary" href="troubleshooting.html">Troubleshooting Logs</a>

        <!-- External -->
        <a class="pill" href="./Hyunwook_Kim_Resume_(040225).pdf" target="_blank" rel="noreferrer">Resume</a>
        <a class="pill" href="https://www.linkedin.com/in/hyunwookkim418/" target="_blank" rel="noreferrer">LinkedIn</a>
      </div>
    </div>
  </div>

  <div class="wrap">
    <div class="hero">
      <h1>Troubleshooting Logs</h1>
    </div>

  <div class="list">
  <div id="pagination" style="margin-top:20px; display:flex; gap:8px; justify-content:center;">
  <button class="btn" id="prevBtn">Prev</button>
  <span id="pageInfo" style="align-self:center; font-weight:800;"></span>
  <button class="btn" id="nextBtn">Next</button>
  </div>


      <!-- Post 25 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-25.html">B 25. Understanding RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Overview of RoBERTa and the key training changes vs BERT, including larger data, longer training, and removing NSP.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-25.html">Read</a>
        </div>
      </div>

      <!-- Post 24 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-24.html">B 24. Understanding BERT's Multi-Head Attention Mechanism</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Clear explanation of multi-head attention in BERT and why parallel attention heads capture richer relationships.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-24.html">Read</a>
        </div>
      </div>

      <!-- Post 23 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-23.html">B 23. Comprehensive Guide to Transformer: Attention is All We Need</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Breakdown of the Transformer architecture: self-attention, positional encoding, and the encoder–decoder flow.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-23.html">Read</a>
        </div>
      </div>

       <!-- Post 25 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-25.html">B 25. Understanding RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Overview of RoBERTa and the key training changes vs BERT, including larger data, longer training, and removing NSP.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-25.html">Read</a>
        </div>
      </div>

      <!-- Post 24 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-24.html">B 24. Understanding BERT's Multi-Head Attention Mechanism</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Clear explanation of multi-head attention in BERT and why parallel attention heads capture richer relationships.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-24.html">Read</a>
        </div>
      </div>

      <!-- Post 23 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-23.html">B 23. Comprehensive Guide to Transformer: Attention is All We Need</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Breakdown of the Transformer architecture: self-attention, positional encoding, and the encoder–decoder flow.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-23.html">Read</a>
        </div>
      </div>

       <!-- Post 25 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-25.html">B 25. Understanding RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Overview of RoBERTa and the key training changes vs BERT, including larger data, longer training, and removing NSP.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-25.html">Read</a>
        </div>
      </div>

      <!-- Post 24 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-24.html">B 24. Understanding BERT's Multi-Head Attention Mechanism</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Clear explanation of multi-head attention in BERT and why parallel attention heads capture richer relationships.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-24.html">Read</a>
        </div>
      </div>

      <!-- Post 23 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-23.html">B 23. Comprehensive Guide to Transformer: Attention is All We Need</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Breakdown of the Transformer architecture: self-attention, positional encoding, and the encoder–decoder flow.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-23.html">Read</a>
        </div>
      </div>

       <!-- Post 25 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-25.html">B 25. Understanding RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Overview of RoBERTa and the key training changes vs BERT, including larger data, longer training, and removing NSP.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-25.html">Read</a>
        </div>
      </div>

      <!-- Post 24 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-24.html">B 24. Understanding BERT's Multi-Head Attention Mechanism</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Clear explanation of multi-head attention in BERT and why parallel attention heads capture richer relationships.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-24.html">Read</a>
        </div>
      </div>

      <!-- Post 23 -->
      <div class="post">
        <div class="row1">
          <h3 class="title">
            <a href="blog-posts/blog-post-23.html">B 23. Comprehensive Guide to Transformer: Attention is All We Need</a>
          </h3>
          <span class="meta"><i></i>NLP</span>
        </div>
        <p class="desc">
          Breakdown of the Transformer architecture: self-attention, positional encoding, and the encoder–decoder flow.
        </p>
        <div class="actions">
          <a class="btn primary" href="blog-posts/blog-post-23.html">Read</a>
        </div>
      </div>

    </div>

    <div class="footer">
      © <script>document.write(new Date().getFullYear())</script> Hyunwook Kim · Troubleshooting Logs
    </div>
  </div>

  <script>
  const postsPerPage = 3;
  const posts = document.querySelectorAll('.post');
  const totalPages = Math.ceil(posts.length / postsPerPage);

  let currentPage = 1;

  function renderPage(page) {
    posts.forEach((post, index) => {
      post.style.display =
        index >= (page - 1) * postsPerPage && index < page * postsPerPage
          ? 'block'
          : 'none';
    });

    document.getElementById('pageInfo').textContent =
      `Page ${page} / ${totalPages}`;

    document.getElementById('prevBtn').disabled = page === 1;
    document.getElementById('nextBtn').disabled = page === totalPages;
  }

  document.getElementById('prevBtn').onclick = () => {
    if (currentPage > 1) renderPage(--currentPage);
  };

  document.getElementById('nextBtn').onclick = () => {
    if (currentPage < totalPages) renderPage(++currentPage);
  };

  renderPage(currentPage);
</script>
</body>
</html>
