<!DOCTYPE html>
<html lang="en">

<head>
    <meta name="title" property="og:title" content="Backpropagation">
    <meta property="og:url" content="https://example.com/backpropagation">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Backpropagation">
    <meta property="og:description" content="Understanding Backpropagation and Gradient Descent in Neural Networks">
    <meta property="og:image" content="https://example.com/image.png">

    <title>Backpropagation</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="ionicons/css/ionicons.min.css" rel="stylesheet">
    <link href="css/animate.min.css" rel="stylesheet">
    <link href="css/aos.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Maven+Pro&display=swap" rel="stylesheet">

    <style>
        body {
            background-color: #f8f9fa;
            color: #343a40;
            font-family: 'Maven Pro', sans-serif;
        }

        h1 {
            margin-top: 20px;
            font-size: 2.5rem;
            text-align: center;
            color: #007bff;
        }

        h2 {
            margin-top: 30px;
            font-size: 2rem;
            color: #343a40;
        }

        p {
            font-size: 1.1rem;
            line-height: 1.6;
        }

        ul {
            margin: 20px 0;
            padding-left: 20px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }

        th, td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f4f4f4;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Backpropagation</h1>

        <h2>Step 1: Feed Forward</h2>
        <ol>
            <li>
                <strong>Inputs and Weights:</strong>
                <p>Given inputs: X1 = 0.5 and X2 = 0.3. Weights are denoted as W1, W2, W3, W4, W5, and W6.</p>
            </li>
            <li>
                <strong>Compute the weighted sum for hidden layer node 1:</strong>
                <p>Z1 = X1 * W1 + X2 * W3 = 0.47.</p>
            </li>
            <li>
                <strong>Apply the activation function (sigmoid) for hidden layer node 1:</strong>
                <p>H1 = sigmoid(Z1) = 0.615.</p>
            </li>
            <li>
                <strong>Compute the weighted sum for hidden layer node 2:</strong>
                <p>Z2 = X1 * W2 + X2 * W4 = 0.582.</p>
            </li>
            <li>
                <strong>Compute the weighted sum for the output layer node:</strong>
                <p>Z3 = H1 * W5 + H2 * W6 = 0.6.</p>
            </li>
            <li>
                <strong>Apply the activation function (sigmoid) for the output node:</strong>
                <p>O1 = sigmoid(Z3) = 0.645.</p>
            </li>
        </ol>

        <h2>Step 2: Cost Calculation</h2>
        <p>
            Compute the Mean Squared Error (MSE):<br>
            MSE is given by: C = (1/n) * Σ (y - ŷ)^2<br>
            Assuming ŷ = 0.645 and C = 0.126, solving for y gives: y ≈ 1.000 or 0.290.
        </p>

        <h2>Step 3: Backpropagation and Gradient Descent</h2>
        <p>In the backpropagation stage, you aim to update the weights of the neural network to minimize the cost function using the gradient descent algorithm. Here's a summary of the process:</p>
        <ol>
            <li>
                <strong>Gradient Descent Update Rule:</strong>
                <p>Wnew = Wold - η * (∂C/∂W)</p>
            </li>
            <li>
                <strong>Computing Gradients Using the Chain Rule:</strong>
                <p>The gradient of C with respect to W is computed using calculus, leveraging the chain rule. For instance, for a weight W5 in the output layer:</p>
                <ul>
                    <li>Calculate the derivative of C with respect to the output node activation O1:</li>
                    <p>∂C/∂O1 = -(y - O1).</p>
                    <li>Calculate the derivative of the activation function with respect to Z3:</li>
                    <p>∂O1/∂Z3 = O1 * (1 - O1).</p>
                    <li>Calculate the derivative of Z3 with respect to W5:</li>
                    <p>∂Z3/∂W5 = H1.</p>
                    <li>Apply the chain rule to compute the gradient of C with respect to W5:</li>
                    <p>∂C/∂W5 = ∂C/∂O1 * ∂O1/∂Z3 * ∂Z3/∂W5 = -(y - O1) * O1 * (1 - O1) * H1.</p>
                </ul>
            </li>
        </ol>

        <h2>What Differentiation Means</h2>
        <p>Differentiating the cost function with respect to weights and biases means calculating how much the cost function would change if you made a small change in those parameters. This helps in adjusting weights to minimize the cost function efficiently.</p>

        <a href="../blogpost.html" class="back-link">Back to Blog</a>
    </div>
</body>

</html>
